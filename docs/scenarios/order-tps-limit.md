# 주문 처리량 한계

## 상황

핫딜 이벤트 시작 시 동시 주문 요청이 폭주하면서 주문 처리량의 한계에 도달한다.
50 VU에서 이미 46req/s로 병목이 발생한다.

## 테스트 환경

- 부하 테스트: `make load-order-stress`
- 최대 VU: 50
- 테스트 대상: `POST /orders` (주문 생성)

---

## 기존 시스템

### 성능

| 지표          | 값      |
| ------------- | ------- |
| p95 응답시간  | 867ms   |
| 평균 응답시간 | 626ms   |
| 처리량        | 46req/s |
| 에러율        | 0%      |

### 리소스 사용량

| 컨테이너 | CPU |
| -------- | --- |
| order    | 86% |
| product  | 40% |

### 문제점

- Order Service CPU 86%로 병목 발생
- 매 주문마다 Product Service로 HTTP 호출
- 50 VU에서 처리량 한계 도달

---

## 개선 시도: httpx 커넥션 풀 재사용

### 변경 내용

- 전역 `httpx.AsyncClient` 생성하여 커넥션 풀 재사용
- `max_connections=100`, `max_keepalive_connections=20` 설정

### 성능

| 지표          | Before  | After   | 개선 |
| ------------- | ------- | ------- | ---- |
| p95 응답시간  | 867ms   | 912ms   | -5%  |
| 평균 응답시간 | 626ms   | 639ms   | -2%  |
| 처리량        | 46req/s | 45req/s | -2%  |
| 에러율        | 0%      | 0%      | -    |

### 분석

- 효과 없음 - Order → Product 통신은 DB 트랜잭션과 함께 처리되어 커넥션 풀 효과 제한적
- HTTP/1.1의 근본적 한계 (Head-of-line blocking)

---

## 개선 1: gRPC 전환

### 변경 내용

- HTTP/JSON → gRPC/Protobuf 전환
- HTTP/2 멀티플렉싱으로 단일 연결에서 다중 요청 동시 처리
- 바이너리 직렬화로 JSON 파싱 오버헤드 제거

### 성능

| 지표          | Before  | After    | 개선    |
| ------------- | ------- | -------- | ------- |
| p95 응답시간  | 867ms   | 367ms    | 2.4배 ↓ |
| 평균 응답시간 | 626ms   | 163ms    | 3.8배 ↓ |
| 처리량        | 46req/s | 125req/s | 2.7배 ↑ |
| 에러율        | 0%      | 0%       | -       |

### 리소스 사용량 비교

| 컨테이너 | Before | After |
| -------- | ------ | ----- |
| order    | 86%    | 64%   |
| product  | 40%    | 79%   |

### 분석

- 처리량 2.7배 증가 (46 → 125req/s)
- 병목이 Order(86%) → Product(79%)로 이동
- Product Service가 새로운 병목 지점

---

## 개선 2: Uvicorn Workers 증가

### 변경 내용

- Order/Product 서비스 모두 2워커로 실행
- 각 워커가 별도 프로세스로 GIL 우회

### 성능

| 지표          | Before   | After    | 개선    |
| ------------- | -------- | -------- | ------- |
| p95 응답시간  | 367ms    | 224ms    | 1.6배 ↓ |
| 평균 응답시간 | 163ms    | 59ms     | 2.8배 ↓ |
| 처리량        | 125req/s | 208req/s | 1.7배 ↑ |
| 에러율        | 0%       | 0%       | -       |

### 리소스 사용량 비교

| 컨테이너 | Before | After |
| -------- | ------ | ----- |
| order    | 64%    | 100%  |
| product  | 79%    | 120%  |

### 분석

- 처리량 67% 증가 (125 → 208req/s)
- CPU 사용량 급증 (Product 79% → 120%)
- Python GIL 우회로 멀티코어 활용

---

## 개선 3: Product Service Go 전환

### 변경 내용

- Product Service를 Go로 완전 재작성
- gRPC 서버 구현 (GetProduct, GetDeal, UpdateStock)

### 성능

| 지표          | Before   | After    | 개선    |
| ------------- | -------- | -------- | ------- |
| p95 응답시간  | 224ms    | 154ms    | 1.5배 ↓ |
| 평균 응답시간 | 59ms     | 31ms     | 1.9배 ↓ |
| 처리량        | 208req/s | 253req/s | 1.2배 ↑ |
| 에러율        | 0%       | 0%       | -       |

### 리소스 사용량 비교

| 컨테이너 | Before | After |
| -------- | ------ | ----- |
| order    | 100%   | 100%  |
| product  | 120%   | 20%   |

### 분석

- 처리량 22% 증가 (208 → 253req/s)
- Product CPU 83% 감소 (120% → 20%)
- Order Service가 새로운 병목 지점

---

## 개선 4: Order Service Go 전환

### 변경 내용

- Order Service를 Go로 완전 재작성
- Echo 프레임워크 기반 HTTP API
- gRPC 클라이언트로 Product Service 호출

### 성능

| 지표          | Before   | After    | 개선    |
| ------------- | -------- | -------- | ------- |
| p95 응답시간  | 154ms    | 20ms     | 7.7배 ↓ |
| 평균 응답시간 | 31ms     | 9ms      | 3.5배 ↓ |
| 처리량        | 253req/s | 295req/s | 1.2배 ↑ |
| 에러율        | 0%       | 0%       | -       |

### 리소스 사용량 비교

| 컨테이너 | Before | After |
| -------- | ------ | ----- |
| order    | 100%   | 15%   |
| product  | 20%    | 16%   |

### 분석

- 처리량 17% 증가 (253 → 295req/s)
- Order CPU 85% 절감 (100% → 15%)
- 두 서비스 모두 Go로 전환되어 리소스 효율성 극대화

---

## 전체 개선 요약

| 단계                | 처리량   | p95 응답 | Product CPU | Order CPU |
| ------------------- | -------- | -------- | ----------- | --------- |
| 기존 (HTTP)         | 46req/s  | 867ms    | 40%         | 86%       |
| 개선 1 (gRPC)       | 125req/s | 367ms    | 79%         | 64%       |
| 개선 2 (2워커)      | 208req/s | 224ms    | 120%        | 100%      |
| 개선 3 (Go Product) | 253req/s | 154ms    | 20%         | 100%      |
| 개선 4 (Go Order)   | 295req/s | 20ms     | 16%         | 15%       |

### 주요 인사이트

- HTTP → gRPC: 처리량 2.7배 증가
- Python 워커 증가: 처리량 67% 증가, CPU 비용 증가
- Product Python → Go: 처리량 22% 증가, Product CPU 83% 절감
- Order Python → Go: 처리량 17% 증가, Order CPU 85% 절감

### 최종 결과: 기존 대비

- 처리량: 46req/s → 295req/s (**6.4배 증가**)
- 응답시간: p95 867ms → 20ms (**43배 감소**)
- CPU 사용량: Order 86%+Product 40% → Order 15%+Product 16% (**75% 절감**)
